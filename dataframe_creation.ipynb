{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import Bio\n",
    "import statsmodels.api as sm\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook can be used to generate the full dataset used in our works, but shouldn't be run because it may take a very long time\n",
    "#### The finished dataframe 'prokaryotes_348columns.csv' is available in the 'data' folder (after running initialisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from computer\n",
    "path: str = \"./data\"  #folder where files are stored\n",
    "species: pd.DataFrame = pd.read_csv(os.path.join(path, \"cross-species.csv\"), dtype=str)   # imports file from the folder as species, based on name\n",
    "identifiers: pd.DataFrame = pd.read_csv(os.path.join(path, \"identifiers.tsv\"), sep='\\t', dtype=str) # imports 3rd file from the folder, based on name\n",
    "prokaryotes_auc : pd.DataFrame = pd.read_csv(os.path.join(path, \"data_prokaryotes_auc.csv\"), dtype=str)\n",
    "Uniprot_ID_mapping : pd.DataFrame = pd.read_csv(os.path.join(path, \"Uniprot_ID_mapping.tsv\"), sep = '\\t', dtype=str)\n",
    "\n",
    "#import fasta file with biopython (Bio)\n",
    "from Bio import SeqIO\n",
    "fasta_id = []\n",
    "fasta_seq = []\n",
    "with open(os.path.join(path, \"identifiers.fasta\"), 'r') as handle:\n",
    "    for record in SeqIO.parse(handle, \"fasta\"): \n",
    "        fasta_id.append(record.id) \n",
    "        fasta_seq.append(str(record.seq))\n",
    "# Create df prot_seq which includes proper ID and sequences\n",
    "fasta_id_df = pd.DataFrame([item.split('|') for item in fasta_id], columns=['0', 'From','ID']) # creates a df by splitting fasta_id list into 3 columns\n",
    "fasta_id_EntryName:list = fasta_id_df['ID'] # creates a list with the Entry Names from the split df\n",
    "prot_seq = pd.DataFrame(list(zip(fasta_id_EntryName, fasta_seq)), columns = [\"ID\", \"Sequence\"]) # creates a df by combining the Entry Names and the Sequences\n",
    "\n",
    "\n",
    "\n",
    "#Define datatype for each column\n",
    "species['fold_change'] = species['fold_change'].astype('float64') # changes fold_change column to float64\n",
    "species['temperature'] = species['temperature'].astype('float64') # changes temperature column to int64\n",
    "identifiers['Length'] = identifiers['Length'].astype('float64') # changes Length column to int64\n",
    "\n",
    "#Update index\n",
    "species = species.reset_index(drop=True) # resets index of species df\n",
    "identifiers = identifiers.reset_index(drop=True) # resets index of identifiers df\n",
    "\n",
    "Uniprot_ID_mapping = Uniprot_ID_mapping.drop(columns = ['From', 'Entry', 'Gene Ontology (cellular component)', 'Gene Ontology (biological process)', '3D', 'Subcellular location [CC]', 'Intramembrane', 'Topological domain', 'Transmembrane'], errors= 'ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add protein sequence from protein_seq dataset to identifiers to create identifiers_seq, based on 'ID' and 'Entry Name' respectively\n",
    "\n",
    "Create joint data set from identifiers_seq and species_seq based on gene_name and Gene Names1 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes based on Entry Name (df.identifiers) andID (df.prot_seq), if they are similar add the sequence to identifiers, if not add NaN\n",
    "identifiers_seq = identifiers.merge(prot_seq, how='left', left_on='Entry Name', right_on='ID')[identifiers.columns.tolist() + ['Sequence']] # merges identifiers and prot_seq on Entry Name and ID\n",
    "identifiers_seq.head()\n",
    "\n",
    "#merge species and identifiers_seq based on gene_name and Gene Names1\n",
    "species[['ProtID1', 'ProtID2']] = species['Protein_ID'].str.split(\"_\", expand = True, n=1) # splits the Gene Names column into 2 columns\n",
    "species_seq = pd.merge(left = species, right = identifiers_seq, how='outer', left_on='ProtID1', right_on='Entry') # merges species and identifiers_seq on gene_name and Gene Names1\n",
    "\n",
    "# drop entrys with no sequence and unnessecary columns\n",
    "species_seq.dropna(subset = ['Sequence'], inplace=True) # drops rows with NaN in the Sequence column\n",
    "species_seq = species_seq.drop(columns=['ProtID1', 'ProtID2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract list of unique run_names from species and manually seperate in prokaryotes and eukaryotes. Create split datasets for eukaryotes and prokaryotes based on species_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of unique entries in the 'run_name' column\n",
    "#print(species['run_name'].unique())\n",
    "\n",
    "# Based on uniqe run names all entries split into eukaryotes and prokaryotes (manually)\n",
    "prokaryotes_list:list = ['Bacillus subtilis_168_lysate_R1', 'Escherichia coli lysate',\n",
    " 'Geobacillus stearothermophilus NCA26 lysate',\n",
    " 'Thermus thermophilus HB27 lysate', 'Thermus thermophilus HB27 cells',\n",
    " 'Escherichia coli cells', 'Picrophilus torridus DSM9790 lysate', 'Oleispira antarctica_RB-8_lysate_R1']\n",
    "\n",
    "#creates new dataframes that only contain prokaryotes or eukaryotes based\n",
    "prokaryotes_all = species_seq[species_seq['run_name'].isin(prokaryotes_list)]\n",
    "\n",
    "##reset index\n",
    "prokaryotes_all = prokaryotes_all.reset_index(drop=True) # resets index of species df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Picrophilus torridus DSM790 lysate',)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prokaryotes_all_list = [\n",
    "    'Thermus thermophilus HB27 lysate',                 #\n",
    "    'Thermus thermophilus HB27 cells',                  #\n",
    "    'Picrophilus torridus DSM9790 lysate',              #\n",
    "    'Bacillus subtilis_168_lysate_R1',                  #\n",
    "    'Escherichia coli lysate',                          #\n",
    "    'Escherichia coli cells',                           #\n",
    "    'Geobacillus stearothermophilus NCA26 lysate',\n",
    "    'Oleispira antarctica_RB-8_lysate_R1'\n",
    "    ]\n",
    "\n",
    "prokaryotes_auc_list = [\n",
    "    'T.thermophilus_P023431',                          # Thermus thermophilus HB27 lysate\n",
    "    'T.thermophilus_cells_P023757',                     # Thermus thermophilus HB27 cells\n",
    "    'P.torridus_P023430',                               # Picrophilus torridus DSM9790 lysate\n",
    "    'B.subtilis_P023755',                               # Bacillus subtilis_168_lysate_R1                                                   \n",
    "    'E.coli_cells_P023756',                             # Escherichia coli lysate\n",
    "    'E.coli_P023428',                                   # Escherichia coli cells\n",
    "    'G.stearothermophilus_P023429',                     # Geobacillus stearothermophilus NCA26 lysate\n",
    "    'O.antarctica_P028248',                             # Oleispira antarctica_RB-8_lysate_R1  \n",
    "]\n",
    "# E.coli_ArcticExpress_P028249 not included\n",
    "\n",
    "#Prokaryotes from R-shiny (https://meltomeatlas.proteomics.wzw.tum.de/master_meltomeatlasapp/)\n",
    "'Escherichia coli lysate', \n",
    "'Escherichia coli cells', \n",
    "'Geobacillus stearothermophilus NCA26 lysate',\n",
    "'Thermus thermophilus HB27 lysate',\n",
    "'Thermus thermophilus HB27 cells',\n",
    "'Picrophilus torridus DSM790 lysate',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust organism name in prokaryotes_auc to match names from prokaryotes_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prokaryotes_auc['Sample'] = prokaryotes_auc['Sample'].replace(dict(zip(prokaryotes_auc_list, prokaryotes_all_list)))\n",
    "\n",
    "#remove E.coli_ArcticExpress_P028249\n",
    "prokaryotes_auc = prokaryotes_auc[prokaryotes_auc['Sample'] != 'E.coli_ArcticExpress_P028249']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine prokaryotes_all dataframe with prokaryotes_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prokaryotes_all['run_name_Protein_ID'] = prokaryotes_all['run_name'] + prokaryotes_all['Protein_ID']\n",
    "prokaryotes_auc['Sample_Protein_ID'] = prokaryotes_auc['Sample'] + prokaryotes_auc['Protein_ID']\n",
    "\n",
    "prokaryotes_all = pd.merge(left = prokaryotes_all, right = prokaryotes_auc , how='left', left_on='run_name_Protein_ID', right_on='Sample_Protein_ID') # merges species and identifiers_seq on gene_name and Gene Names1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop unessesary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns\n",
    "col_drop = ['Reviewed', 'run_name_Protein_ID', 'channel', 'From', 'Protein_ID_y',\n",
    "            'Proteinname_P023428_E.coli', 'meltPoint_P023428_E.coli', 'gene_name_y',\n",
    "            'Sample_Protein_ID', 'Sample', 'Entry', 'Gene Names', 'uniprot_ac']\n",
    "for n in range(len(col_drop)):\n",
    "    if col_drop[n] in list(prokaryotes_all.columns):\n",
    "        prokaryotes_all = prokaryotes_all.drop(columns=[col_drop[n]])\n",
    "\n",
    "#rename columns\n",
    "prokaryotes_all = prokaryotes_all.rename(columns={'Protein_ID_x': 'Protein_ID'})\n",
    "prokaryotes_all = prokaryotes_all.rename(columns={'gene_name_x': 'gene_name'})\n",
    "\n",
    "#define variable\n",
    "prokaryotes_all = prokaryotes_all.astype({'auc': 'float64'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Uniprot_ID_mapping with prokaryotes dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prokaryotes_all = pd.merge(left = prokaryotes_all, right = Uniprot_ID_mapping, how='left', left_on='Entry Name', right_on='Entry Name') # merges species and identifiers_seq on gene_name and Gene Names1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-order dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_ordered =['run_name', 'Organism', 'Protein_ID' ,'Entry Name' ,'gene_name', \n",
    " 'Protein names',  'Temperature dependence', 'Length', 'Sequence', \n",
    " 'temperature', 'fold_change', 'meltPoint', 'auc', 'Gene Ontology IDs', \n",
    " 'Gene Ontology (biological process)', 'Gene Ontology (cellular component)', \n",
    " 'Gene Ontology (molecular function)', 'KEGG','EC number', 'Helix', 'Turn', 'Beta strand',\n",
    "       'AlphaFoldDB', 'PDB']\n",
    "\n",
    "prokaryotes_all = prokaryotes_all[columns_ordered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of each unique protein for each organism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prokaryotes\n",
    "prokaryotes = prokaryotes_all.drop_duplicates(subset=['run_name', 'Protein_ID'], keep='first') #create new dataframe with only unique proteins\n",
    "prokaryotes = prokaryotes.reset_index(drop=True) #reset index\n",
    "\n",
    "# define meltpoint as float64\n",
    "prokaryotes['meltPoint'] = prokaryotes['meltPoint'].astype('float64') # changes meltpoint column to float64\n",
    "prokaryotes['temperature'] = prokaryotes['temperature'].astype('float64')\n",
    "prokaryotes['fold_change'] = prokaryotes['fold_change'].astype('float64')\n",
    "prokaryotes['Length'] = prokaryotes['Length'].astype('float64')\n",
    "prokaryotes['auc'] = prokaryotes['auc'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prokaryotes_all.to_csv(os.path.join(path,'prokaryotes_all.csv'), index=False)\n",
    "prokaryotes.to_csv(os.path.join(path,'prokaryotes_unique_prot.csv'), index=False)\n",
    "prokaryotes['Entry Name'].to_csv(os.path.join(path, 'Uniprot_IDs.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read fasta file with all secondary structure data from s4pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2: str = f\"{os.path.abspath(os.path.join(os.getcwd()))}/data/sec-structure_prediction.fas\"\n",
    "with open(path2, 'r') as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list with secondary structure data for all proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contentl=content.split('#')\n",
    "contentl.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import prokaryotes dataframe from csv and add columns for secondary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = \"./data\"\n",
    "prokaryotes: pd.DataFrame = pd.read_csv(os.path.join(path, \"prokaryotes_unique_prot.csv\"), dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns for secondary structures\n",
    "prokaryotes['Helix1']=pd.Series(dtype=object)\n",
    "prokaryotes['Turn1']=pd.Series(dtype=object)\n",
    "prokaryotes['Sheet1']=pd.Series(dtype=object)\n",
    "prokaryotes['Helix2']=pd.Series(dtype=object)\n",
    "prokaryotes['Coil2']=pd.Series(dtype=object)\n",
    "prokaryotes['Sheet2']=pd.Series(dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding columns with secondary structure as lists of lists **(only ones from crystal structure)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex patterns for following steps\n",
    "pattern = r'\\b\\d+\\.\\.\\d+\\b'\n",
    "pattern2 = r'\\b\\d+\\b'\n",
    "#filling helix1 column\n",
    "for q in range(len(prokaryotes)):\n",
    "    templist = []\n",
    "    if pd.isnull(prokaryotes.loc[q, 'Helix']) == False:\n",
    "        tempt = re.findall(pattern, prokaryotes.loc[q, 'Helix'])\n",
    "        for m in range(len(tempt)):    \n",
    "            tempt2 = re.findall(pattern2, tempt[m])\n",
    "            tempt2 = list(map(int, tempt2))\n",
    "            templist.append(list(range(tempt2[0], tempt2[1]+1)))\n",
    "        prokaryotes.at[q,'Helix1'] = templist\n",
    "    else: \n",
    "        prokaryotes.at[q, 'Helix1'] = np.NaN\n",
    "#filling Turn1 column\n",
    "for w in range(len(prokaryotes)):\n",
    "    templist = []\n",
    "    if pd.isnull(prokaryotes.loc[w, 'Turn']) == False:\n",
    "        tempt = re.findall(pattern, prokaryotes.loc[w, 'Turn'])\n",
    "        for m in range(len(tempt)):    \n",
    "            tempt2 = re.findall(pattern2, tempt[m])\n",
    "            tempt2 = list(map(int, tempt2))\n",
    "            templist.append(list(range(tempt2[0], tempt2[1]+1)))\n",
    "        prokaryotes.at[w,'Turn1'] = templist\n",
    "    else: \n",
    "        prokaryotes.at[w, 'Turn1'] = np.NaN\n",
    "#filling Sheet1 column\n",
    "for w in range(len(prokaryotes)):\n",
    "    templist = []\n",
    "    if pd.isnull(prokaryotes.loc[w, 'Beta strand']) == False:\n",
    "        tempt = re.findall(pattern, prokaryotes.loc[w, 'Beta strand'])\n",
    "        for m in range(len(tempt)):    \n",
    "            tempt2 = re.findall(pattern2, tempt[m])\n",
    "            tempt2 = list(map(int, tempt2))\n",
    "            templist.append(list(range(tempt2[0], tempt2[1]+1)))\n",
    "        prokaryotes.at[w,'Sheet1'] = templist\n",
    "    else: \n",
    "        prokaryotes.at[w, 'Sheet1'] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding columns with secondary structure from S4pred predictions for **all** proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(contentl)):\n",
    "    pat = r'\\d+\\s\\w\\s\\w'\n",
    "    patl = re.findall(pat, contentl[n])\n",
    "    for p in range(len(patl)):\n",
    "        patl[p] = patl[p].split(' ')\n",
    "    helic = []\n",
    "    sheet = []\n",
    "    coil = []\n",
    "    for k in range(len(patl)):\n",
    "        if patl[k][2] == 'H':\n",
    "            helic.append(int(patl[k][0]))\n",
    "        elif patl[k][2] == 'E':\n",
    "            sheet.append(int(patl[k][0]))\n",
    "        elif patl[k][2] == 'C':\n",
    "            coil.append(int(patl[k][0]))\n",
    "    helices = []\n",
    "    sheets = []\n",
    "    coils = []\n",
    "    for val in helic:\n",
    "        if val-1 not in helic:\n",
    "            lower = val\n",
    "        if val+1 not in helic:\n",
    "            upper = val\n",
    "            helices.append(list(range(lower,upper+1)))\n",
    "    for coi in coil:\n",
    "        if coi-1 not in coil:\n",
    "            lower = coi\n",
    "        if coi+1 not in coil:\n",
    "            upper = coi\n",
    "            coils.append(list(range(lower,upper+1)))\n",
    "    for she in sheet:\n",
    "        if she-1 not in sheet:\n",
    "            lower = she\n",
    "        if she+1 not in sheet:\n",
    "            upper = she\n",
    "            sheets.append(list(range(lower,upper+1)))\n",
    "    prokaryotes.at[n, 'Helix2'] = helices\n",
    "    prokaryotes.at[n, 'Coil2'] = coils\n",
    "    prokaryotes.at[n, 'Sheet2'] = sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate lysate and cell data\n",
    "\n",
    "prokaryotes = prokaryotes[prokaryotes['run_name'].str.contains('lysate', case=True)].dropna(subset=['meltPoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prokaryotes.to_csv(os.path.join(path,'prokaryotes_sec_structure.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = './data/'\n",
    "prokaryotes: pd.DataFrame = pd.read_csv(os.path.join(path, \"prokaryotes_sec_structure.csv\"))\n",
    "prokaryotes = prokaryotes[prokaryotes['run_name'].str.contains('lysate', case=True)].dropna(subset=['meltPoint'])\n",
    "secs = ['Helix1','Turn1','Sheet1','Helix2','Sheet2']\n",
    "# convert string to list because list information was while saving as csv\n",
    "if isinstance(prokaryotes.iloc[0,28],str):\n",
    "    for s in secs:\n",
    "        prokaryotes[s] = prokaryotes[s].apply(lambda x: ast.literal_eval(x) if pd.isnull(x)==False else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding columns for total number of helices, sheets and coils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in secs:\n",
    "    prokaryotes[f'{s}count'] = prokaryotes[s].apply(lambda x: len(x) if isinstance(x, list) else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding columns for relative Helix and Sheet abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(prokaryotes['Length'])\n",
    "for s in secs:\n",
    "    prokaryotes[f'{s}perc'] = prokaryotes[s].apply(lambda x: pd.Series(x).map(len).sum() if isinstance(x, list) else np.nan)\n",
    "    prokaryotes[f'{s}perc'] = np.array(prokaryotes[f'{s}perc'])/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding columns for average Helix and Sheet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in secs:\n",
    "    prokaryotes[f'{s}avg'] = prokaryotes[s].apply(lambda x: np.array([len(lst) for lst in np.array(x,dtype=object)]).mean() if isinstance(x, list) and len(x)>0 else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding column for relative fraction of secondary structures (Helix and Beta sheet combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prokaryotes['secstr1'] = np.array(prokaryotes['Helix1perc']) + np.array(prokaryotes['Sheet1perc'])\n",
    "prokaryotes['secstr2'] = np.array(prokaryotes['Helix2perc']) + np.array(prokaryotes['Sheet2perc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "aacids = ['A', 'V', 'I', 'L', 'M', 'F', 'W','N', 'Q', 'S', 'T', 'Y','D', 'E','R', 'H', 'K', 'C', 'P', 'G' ]\n",
    "from function import rel_aa_comp\n",
    "aagl = []\n",
    "for n in aacids:\n",
    "    for m in aacids:\n",
    "        if n != m and [m,n] not in aagl:\n",
    "            aagl.append([n,m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in aagl:\n",
    "    prokaryotes[f'{g[0]}{g[1]}'] = prokaryotes['Sequence'].apply(lambda x: rel_aa_comp(x,[g[0],g[1]]))\n",
    "    if abs(prokaryotes[f'{g[0]}{g[1]}'].corr(prokaryotes['meltPoint'])) < 0.2:  #Threshold for correlation\n",
    "        prokaryotes = prokaryotes.drop(columns = [f'{g[0]}{g[1]}']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating amino acid percentage inside helices and sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "test = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "tests = str(list(itertools.chain.from_iterable(test)))\n",
    "print(type(tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prokaryotes['helixind'] = prokaryotes['Helix2'].apply(lambda x: list(np.concatenate(np.array(x,dtype=object))) if len(x) > 0 else [])\n",
    "prokaryotes['helixseq'] = prokaryotes.apply(lambda row: [row['Sequence'][i] for i in row['helixind'] if i < len(row['Sequence'])], axis=1)\n",
    "for a in aacids:\n",
    "    prokaryotes[f'{a}helix'] = prokaryotes['helixseq'].apply(lambda x: x.count(a)/len(x) if len(x) > 0 else np.nan) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_val(corr, n, alpha):\n",
    "    import math\n",
    "    import scipy.stats as stats\n",
    "    if math.sqrt((1-(corr**2))/(n-2)) != 0 and n-2 != 0:\n",
    "        t = (corr)/(math.sqrt((1-(corr**2))/(n-2)))\n",
    "        p = 1 - stats.t.cdf(t, n-2)\n",
    "        return [p, p < alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in aacids:\n",
    "    prokaryotes[str(a)] = prokaryotes['Sequence'].apply(lambda x: x.count(a)/len(x) if len(x) > 0 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "aat = []\n",
    "for a in aacids:\n",
    "    for b in aacids:\n",
    "        aat.append(f'{a}{b}')\n",
    "\n",
    "for a in aat:\n",
    "    prokaryotes[f'{a}motif'] = prokaryotes['Sequence'].apply(lambda x: x.count(a)/len(x) if len(x) > 0 else np.nan)\n",
    "    if abs(prokaryotes[f'{a}motif'].corr(prokaryotes['meltPoint'])) < 0.1:  #Threshold for correlation\n",
    "        prokaryotes = prokaryotes.drop(columns = [f'{a}motif']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prokaryotes['HydrophobicAA'] = prokaryotes['Sequence'].apply(lambda x: rel_aa_comp(x,['A','V','I','L','M','F','W']))\n",
    "prokaryotes['ChargedAA'] = prokaryotes['Sequence'].apply(lambda x: rel_aa_comp(x,['R','H','K','D','E']))\n",
    "prokaryotes['PolarAA'] = prokaryotes['Sequence'].apply(lambda x: rel_aa_comp(x,['N','Q','S','T','Y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDB data import from AlphaFold database (chunk takes a long time because ~ 5000 pdb files are downloaded, download link is provided in README, but 1 GB of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_15344\\3021300851.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inp[i] = inp[i].replace(';','')\n",
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_15344\\3021300851.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inp[i] = f'AF-{inp[i]}-F1'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m path_finalpdb \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(target_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malphafold_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://alphafold.ebi.ac.uk/files/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malphafold_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatabase_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdb\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m---> 23\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurl \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -o \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_finalpdb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path: str = \"./data\" #folder where files are stored\n",
    "path2 = f\"{os.path.abspath(os.path.join(os.getcwd(), '..'))}\\data\\pdbs\"\n",
    "prokaryotes: pd.DataFrame = pd.read_csv(os.path.join(path, \"prokaryotes_unique_prot.csv\"), dtype=str)   \n",
    "prokaryotes2 = prokaryotes.dropna(subset=['AlphaFoldDB'])\n",
    "tzui = []\n",
    "for m in range(len(prokaryotes2)):\n",
    "    if pd.isnull(prokaryotes.iloc[m,23]) == True:\n",
    "        tzui.append(m)\n",
    "prokaryotes3 = prokaryotes2.iloc[tzui,:]\n",
    "inp = prokaryotes3['AlphaFoldDB']\n",
    "inp.reset_index(drop=True, inplace=True)\n",
    "for i in range(len(inp)):\n",
    "    inp[i] = inp[i].replace(';','')\n",
    "    inp[i] = f'AF-{inp[i]}-F1'\n",
    "database_version = 'v4'\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "target_dir = os.path.join(base_dir, 'data', 'pdbs')\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "for o in range(len(inp)):\n",
    "    alphafold_ID = inp[o]\n",
    "    path_finalpdb = os.path.join(target_dir, f'{alphafold_ID}.pdb')\n",
    "    url = f'https://alphafold.ebi.ac.uk/files/{alphafold_ID}-model_{database_version}.pdb' \n",
    "    os.system(f'curl {url} -o \"{path_finalpdb}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration of tertiary structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\OneDrive\\Documents\\Uni\\FS 4\\Bioinfo Projekt\\topic04_02\\archive\\helper_function.py:58: RuntimeWarning: invalid value encountered in cast\n",
      "  distance[:,0] = distance[:,0].astype('int')\n"
     ]
    }
   ],
   "source": [
    "from function import salt_bridge\n",
    "path = './data/pdbs'\n",
    "Salt_bridges = salt_bridge(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I calculate the amount of brdiges as well as the sum of the bridge distances within the saltbridge dictionary\n",
    "test = Salt_bridges['P10943']\n",
    "amount_bridges = {}\n",
    "for n, k in Salt_bridges.items():\n",
    "    amount_bridges[n] = np.sum(~np.isnan(k)) - sum(Salt_bridges[n].shape) +2\n",
    "sum_bridges = {}\n",
    "for n, k in Salt_bridges.items():\n",
    "    sum_bridges[n] = np.nansum(k) - np.nansum(Salt_bridges[n][0,:]) - np.nansum(Salt_bridges[n][:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(prokaryotes)):\n",
    "    if pd.isnull(prokaryotes.loc[n,'AlphaFoldDB']) == False:\n",
    "        if prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')  in amount_bridges:\n",
    "            prokaryotes.loc[n, 'A_Salty'] = amount_bridges[prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')]\n",
    "\n",
    "for n in range(len(prokaryotes)):\n",
    "    if pd.isnull(prokaryotes.loc[n,'AlphaFoldDB']) == False:\n",
    "        if prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')  in sum_bridges:\n",
    "            prokaryotes.loc[n, 'S_Salty'] = sum_bridges[prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean salt bridge length per protein and append it to the df\n",
    "M_Salty = []\n",
    "\n",
    "for n in range(len(prokaryotes)):\n",
    "    if pd.isnull(prokaryotes.loc[n, 'S_Salty']) == False and pd.isnull(prokaryotes.loc[n, 'A_Salty']) == False and prokaryotes.loc[n, 'S_Salty'] !=0 and prokaryotes.loc[n, 'A_Salty'] !=0:\n",
    "        M_Salty = (prokaryotes.loc[n, 'S_Salty']/prokaryotes.loc[n, 'A_Salty'])\n",
    "        prokaryotes.loc[n, 'M_Salty'] = M_Salty\n",
    "    elif pd.isnull(prokaryotes.loc[n, 'S_Salty']) == True:\n",
    "        prokaryotes.loc[n, 'M_Salty'] = np.nan\n",
    "    elif pd.isnull(prokaryotes.loc[n, 'A_Salty']) == True:\n",
    "        prokaryotes.loc[n, 'M_Salty'] = np.nan\n",
    "    elif prokaryotes.loc[n, 'S_Salty'] ==0:\n",
    "        prokaryotes.loc[n, 'M_Salty'] = np.nan\n",
    "    elif prokaryotes.loc[n, 'A_Salty'] ==0:\n",
    "        prokaryotes.loc[n, 'M_Salty'] = np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hydrophobic patches - integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function import VdW_interaction\n",
    "VdW_clus, VdW_vol = VdW_interaction('./data/pdbs' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate amount of clusters/hydrophobic regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the amount of cluster\n",
    "amount_cluster = {}\n",
    "\n",
    "for n,k in VdW_clus.items():\n",
    "        amount_cluster[n] = len(k)\n",
    "\n",
    "length_cluster = {}\n",
    "cluster_list = []\n",
    "\n",
    "#calculate the lenght of the lists i.e. the amount of AS per cluster/hydrophobic patch\n",
    "def calculate_list_lengths(VdW_clus):\n",
    "    length_cluster = {}\n",
    "    for key, value in VdW_clus.items():\n",
    "        total_length = 0\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            for sub_key, sub_value in value.items():\n",
    "                    total_length += len(sub_value)\n",
    "        \n",
    "        length_cluster[key] = total_length\n",
    "    \n",
    "    return length_cluster\n",
    "\n",
    "length_cluster = calculate_list_lengths(VdW_clus)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now concat the dictionaries onto the prokaryote df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(prokaryotes)):\n",
    "    if pd.isnull(prokaryotes.loc[n,'AlphaFoldDB']) == False:\n",
    "        if prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')  in amount_cluster:\n",
    "            prokaryotes.loc[n, 'Amount_Cluster'] = amount_cluster[prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')]\n",
    "\n",
    "#now divide amount of clusters by amino acid sequence length\n",
    "prokaryotes['Amount_Cluster'] = pd.to_numeric(prokaryotes['Amount_Cluster'], errors='coerce')\n",
    "prokaryotes['Length'] = pd.to_numeric(prokaryotes['Length'], errors='coerce')\n",
    "prokaryotes['relative_Amount_Cluster'] = prokaryotes['Amount_Cluster'] / prokaryotes['Length']\n",
    "for n in range(len(prokaryotes)):\n",
    "    prokaryotes.loc[n, 'relative_Amount_Cluster'] = prokaryotes.loc[n, 'Amount_Cluster']/prokaryotes.loc[n, 'Length']\n",
    "\n",
    "prokaryotes['Cluster_length'] = pd.Series(dtype=object)\n",
    "for n in range(len(prokaryotes)):\n",
    "    if pd.isnull(prokaryotes.loc[n,'AlphaFoldDB']) == False and prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')  in length_cluster:\n",
    "        prokaryotes.loc[n, 'Cluster_length'] = length_cluster[prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')]\n",
    "    prokaryotes.loc[n, 'relative_Cluster_length'] = prokaryotes.loc[n, 'Cluster_length']/prokaryotes.loc[n, 'Length']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now use VdW_vol to calculate the relative total overlapping volume and further data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_15344\\2538635119.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prokaryotes.loc[n, 'Overlapping_Volume_by_Overlapping_AS'] = prokaryotes.loc[n, 'Overlapping_Volume']/prokaryotes.loc[n, 'Overlapping_AS']\n",
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_15344\\2538635119.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prokaryotes.loc[n, 'relative_Overlapping_Volume_by_Overlapping_AS'] = (prokaryotes.loc[n, 'Overlapping_Volume']/prokaryotes.loc[n, 'Overlapping_AS'])/prokaryotes.loc[n, 'Length']\n",
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_15344\\2538635119.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prokaryotes.loc[n, 'Overlapping_Volume_by_Overlapping_AS'] = prokaryotes.loc[n, 'Overlapping_Volume']/prokaryotes.loc[n, 'Overlapping_AS']\n",
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_15344\\2538635119.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prokaryotes.loc[n, 'relative_Overlapping_Volume_by_Overlapping_AS'] = (prokaryotes.loc[n, 'Overlapping_Volume']/prokaryotes.loc[n, 'Overlapping_AS'])/prokaryotes.loc[n, 'Length']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#calculating sum of the overlapping volume\n",
    "sum_volumes = {}\n",
    "for n, k in VdW_vol.items():\n",
    "    sum_volumes[n] = np.nansum(k) - np.nansum(VdW_vol[n][0,:]) - np.nansum(VdW_vol[n][:,0])\n",
    "\n",
    "\n",
    "\n",
    "#calculating the amount of overlapping points \n",
    "amount_overlapping = {}\n",
    "for n, k in VdW_vol.items():\n",
    "    amount_overlapping[n] = np.sum(~np.isnan(k)) - sum(VdW_vol[n].shape) +2\n",
    "\n",
    "\n",
    "\n",
    "#add the values to prokaryotes\n",
    "prokaryotes['Overlapping_Volume'] = pd.Series(dtype=object)\n",
    "for n in range(len(prokaryotes)):\n",
    "    if pd.isnull(prokaryotes.loc[n,'AlphaFoldDB']) == False and prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')  in sum_bridges:\n",
    "            prokaryotes.loc[n, 'Overlapping_Volume'] = sum_volumes[prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')]\n",
    "    prokaryotes.loc[n, 'relative_Overlapping_Volume'] = prokaryotes.loc[n, 'Overlapping_Volume']/prokaryotes.loc[n, 'Length']\n",
    "\n",
    "prokaryotes['relative_Overlapping_AS'] = pd.Series(dtype=object)\n",
    "prokaryotes['Overlapping_AS'] = pd.Series(dtype=object)\n",
    "for n in range(len(prokaryotes)):\n",
    "    if pd.isnull(prokaryotes.loc[n,'AlphaFoldDB']) == False and prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')  in amount_bridges:\n",
    "            prokaryotes.loc[n, 'Overlapping_AS'] = amount_overlapping[prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')]\n",
    "    prokaryotes.loc[n, 'relative_Overlapping_AS'] = prokaryotes.loc[n, 'Overlapping_AS']/prokaryotes.loc[n, 'Length']\n",
    "\n",
    "\n",
    "for n in range(len(prokaryotes)):\n",
    "    prokaryotes.loc[n, 'Overlapping_Volume_by_Overlapping_AS'] = prokaryotes.loc[n, 'Overlapping_Volume']/prokaryotes.loc[n, 'Overlapping_AS']\n",
    "    prokaryotes.loc[n, 'relative_Overlapping_Volume_by_Overlapping_AS'] = (prokaryotes.loc[n, 'Overlapping_Volume']/prokaryotes.loc[n, 'Overlapping_AS'])/prokaryotes.loc[n, 'Length']\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from function import p_val\n",
    "oligolength = 4       #lengths of oligos to test, found for 3,4\n",
    "\n",
    "sequences = prokaryotes['Sequence']\n",
    "meltPoints = prokaryotes['meltPoint']\n",
    "\n",
    "oligo_counts = {}\n",
    "\n",
    "for seq in sequences:                               # Count oligos in sequences and assign count to dictionary\n",
    "    for i in range(len(seq) - oligolength + 1):\n",
    "        oligo = seq[i:i + oligolength]\n",
    "        if oligo in oligo_counts:\n",
    "            oligo_counts[oligo] += 1\n",
    "        else:\n",
    "            oligo_counts[oligo] = 1\n",
    "\n",
    "filtered_oligos = {oligo: count for oligo, count in oligo_counts.items() if count > 20} # Filter oligos with count > 20\n",
    "\n",
    "for oligo in filtered_oligos.keys():\n",
    "    oligo_series = sequences.apply(lambda x: x.count(oligo))\n",
    "    oligocorr = oligo_series.corr(meltPoints)\n",
    "    if abs(oligocorr) > 0.2 and p_val(oligocorr, len(prokaryotes), 0.05)[1] == True:     #Filter only oligos with correlation > 0.2 and p-value < 0.05\n",
    "        prokaryotes[f'{oligo}motif'] = oligo_series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = './data/'\n",
    "prokaryotes.to_csv(os.path.join(path, \"prokaryotes_322columns.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hydrogen bond processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\OneDrive\\Documents\\Uni\\FS 4\\Bioinfo Projekt\\topic04_02\\archive\\helper_function.py:58: RuntimeWarning: invalid value encountered in cast\n",
      "  distance[:,0] = distance[:,0].astype('int')\n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Uni\\FS 4\\Bioinfo Projekt\\topic04_02\\archive\\helper_function.py:76: RuntimeWarning: invalid value encountered in arccos\n",
      "  theta = np.arccos((d_DH[n,1,1]**2 + d_HA[n,1:]**2 - d_DA[1:,1:][n,:]**2)/(2*d_DH[n,1,1]*d_HA[1:,1:][n,:]))\n"
     ]
    }
   ],
   "source": [
    "from function import H_bond_calc\n",
    "\n",
    "path = './data/pqrs'\n",
    "Hydrogen_bonds = H_bond_calc(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pm/vzcy9py91xzg_z5d80js6mtc0000gn/T/ipykernel_57710/3342353473.py:2: DtypeWarning: Columns (6,19,20,21,23,24,25,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  prokaryotes: pd.DataFrame = pd.read_csv(os.path.join(path, \"prokaryotes_322columns.csv\"))\n"
     ]
    }
   ],
   "source": [
    "path: str = './data/'\n",
    "prokaryotes: pd.DataFrame = pd.read_csv(os.path.join(path, \"prokaryotes_322columns.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_bond_count = {}\n",
    "for key, value in Hydrogen_bonds.items():\n",
    "    count = np.count_nonzero(~np.isnan(Hydrogen_bonds[key][1:,1:,0]))\n",
    "    H_bond_count[str(key)] = int(count)\n",
    "\n",
    "\n",
    "for n in range(len(prokaryotes)):\n",
    "    if pd.isnull(prokaryotes.loc[n,'AlphaFoldDB']) == False and prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')  in H_bond_count:\n",
    "        prokaryotes.loc[n, 'H_Bonds'] = H_bond_count[prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')];\n",
    "        prokaryotes.loc[n, 'relative_H_bond_count'] = prokaryotes.loc[n, 'H_Bonds']/prokaryotes.loc[n, 'Length'];\n",
    "        prokaryotes.loc[n, 'H_bond_count_by_Asalty'] = prokaryotes.loc[n, 'H_Bonds']/prokaryotes.loc[n, 'A_Salty'];\n",
    "        prokaryotes.loc[n, 'relative_H_bond_by_Ssalty'] = prokaryotes.loc[n, 'H_Bonds']/prokaryotes.loc[n, 'S_Salty'];\n",
    "        prokaryotes.loc[n, 'relative_H_bond_by_overlapping_as'] = prokaryotes.loc[n, 'H_Bonds']/prokaryotes.loc[n, 'Overlapping_AS'];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function import SASA_calc\n",
    "path = './data/pdbs'\n",
    "Surface_area = SASA_calc(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(prokaryotes)):\n",
    "    if pd.isnull(prokaryotes.loc[n,'AlphaFoldDB']) == False and prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')  in Surface_area:\n",
    "        prokaryotes.loc[n, 'Surface_Area'] = Surface_area[prokaryotes.loc[n,'AlphaFoldDB'].replace(';','')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = './data/'\n",
    "prokaryotes.to_csv(os.path.join(path, 'prokaryotes_323columns.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing including meltPoint and addition of Sheet amino acid percentages and fix of helix amino acid percentages because S4pred counts from 1, python from 0\n",
    "while os.path.basename(os.getcwd()) != 'topic04_02':\n",
    "    os.chdir('..')\n",
    "    print(os.getcwd())\n",
    "path = './data'\n",
    "prokaryotes = pd.read_csv(os.path.join(path, \"prokaryotes_323columns.csv\"))\n",
    "droplist = [0,1,2,3,4,5,6,7,8,10,11,12,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,33,38,43,-1,-2,-3,-4,-5,-6,-12,-13,-14,-15,-16,-17,-18,-19,20,-21,-22,-23,-24]\n",
    "col = prokaryotes.columns\n",
    "Sheetind = pd.Series(np.zeros(prokaryotes.shape[0]))\n",
    "Sheetind = prokaryotes['Sheet2'].apply(lambda x: list(map(int, re.findall(r'\\d+', x) )) if type(x) == str else x)\n",
    "Sheetseq = pd.Series(np.zeros(prokaryotes.shape[0]))\n",
    "for n in range(len(Sheetseq)):\n",
    "    Sheetseq[n] = np.array(list(prokaryotes.loc[n,'Sequence']))[Sheetind[n]]\n",
    "prokaryotes = prokaryotes.drop(columns = ['helixind','helixseq'])\n",
    "aacid = ['A', 'V', 'L', 'I', 'P', 'F', 'W', 'M', 'G', 'S', 'C', 'T', 'Y', 'N', 'Q', 'D', 'E', 'K', 'R', 'H']\n",
    "for a in aacid:\n",
    "    prokaryotes.drop(columns = f'{a}helix', inplace = True)\n",
    "helixind = pd.Series(np.zeros(prokaryotes.shape[0]))\n",
    "helixind = prokaryotes['Helix2'].apply(lambda x: np.array(list(map(int, re.findall(r'\\d+', x) ))) -1 if type(x) == str else x)\n",
    "for n in range(len(helixind)):\n",
    "    if len(helixind[n]) != 0:\n",
    "        prokaryotes.at[n,'helixseq'] = np.array(list(prokaryotes.loc[n,'Sequence']))[helixind[n]]\n",
    "for a in aacid:\n",
    "    prokaryotes[f'{a}helix'] = prokaryotes['helixseq'].apply(lambda x: list(x).count(a)/len(x) if type(x) == np.ndarray and np.ndim(x)!=0 else np.nan)\n",
    "Sheetind = pd.Series(np.zeros(prokaryotes.shape[0]))\n",
    "Sheetind = prokaryotes['Sheet2'].apply(lambda x: np.array(list(map(int, re.findall(r'\\d+', x) ))) -1 if type(x) == str else x)\n",
    "for n in range(len(Sheetseq)):\n",
    "    if len(Sheetind[n])!=0:\n",
    "        prokaryotes.at[n,'sheetseq'] = np.array(list(prokaryotes.loc[n,'Sequence']))[Sheetind[n]]\n",
    "for a in aacid:\n",
    "    prokaryotes[f'{a}sheet'] = prokaryotes['sheetseq'].apply(lambda x: list(x).count(a)/len(x) if type(x) == np.ndarray and np.ndim(x)!=0  and len(x) != 0 else np.nan)\n",
    "if 'Unnamed: 0' in prokaryotes.columns:\n",
    "    prokaryotes.drop(columns = ['Unnamed: 0.1','Unnamed: 0'], inplace = True)\n",
    "prokaryotes.to_csv(os.path.join(path, \"prokaryotes_348columns.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prokaryotes_348columns.csv was the final dataframe used in most things afterwards. It includes 348 features of 6558 proteins and also other information like UniprotIDs and other miscellaneous things."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
